## 文章信息
1. Paper: Is Word Segmentation Necessary for Deep Learning of Chinese Representations?
2. ACL 2019

## Intro
1. 分词会带来数据稀疏的问题，同时普遍存在的OOV词限制了模型的学习能力
2. 目前最好的分词工具的分词效果也远远谈不上完美。分词带来的错误会影响到下游的NLP任务
## 实验
1. 语言建模：给定上下文，预测出下一个将要出现的词。
2. 机器翻译：实验了Ch-En和En-Ch两种翻译任务，Seq2Seq+Attention模型
3. 句子匹配
4. 文本分类
	- ![[Pasted image 20201123184955.png]]

## 分析
1. 数据稀疏
	- 分析导致数据更稀疏
	- 避免词汇量变得太大的一种常见方法是设置词频阈值，并使用特殊的UNK标记来表示词频低于阈值的所有单词。词频阈值与词汇量密切相关，因此与参数大小密切相关。下图展示了词频阈值和词汇量的关系，同时也展示了词频阈值和模型表现的关系。
	- ![[Pasted image 20201123185047.png]]
	- 由结果可见，对于两个模型而言，当词频阈值为0（使用全部词语/字符）时效果较差。而对基于字符的模型而言，当词频阈值为5时，词表大小为1432，平均词频为72，此时能取得最佳效果。而对基于词的模型而言，当阈值为50，词表大小为1355，平均词频为83，取得最佳效果。这就意味着，给定一个数据集，为了充分地学习到语义表示，两个模型所见到的数据量应该大致相同。而由于基于词的模型的稀疏性，这一目标通常很难达到
2. OOV
	- 关于基于词的模型表现较差，一种可能的解释就是，它包含了太多的OOV词。如果是这样的话，我们可以通过减少OOV词的数量来减少基于词的模型和基于字符的模型之间的差距。然而，通过这种方式来减少OOV词的数量，也会导致数据稀疏的问题。于是作者采取了一种替代的方式——作者从数据集中删除了所有包含OOV词的句子。下图展示了在词频增加时效果的改变和训练集大小的改变。
	- ![[Pasted image 20201123185141.png]]
	- 可见，在增加词频阈值后，两者的差距逐渐缩小。注意到，基于字符的模型在增加词频阈值后，一开始效果有略微地上升，然后缓慢减小。这是因为对于基于字符的模型而言，OOV问题并不严重，因此不会对性能造成太大影响。但是，随着我们删除越来越多的训练样本，不断缩小的训练集会产生很大的问题。相比之下，对于基于词的模型而言，即使将频率阈值设置为50，性能也会继续提高，这意味着删除一些OOV词的积极影响要大于删除一些训练数据带来的消极影响。总之，基于词的模型存在严重的OOV问题。 可以通过减少数据集中的OOV词的数量来缓解此问题。
3. 过拟合
	- 数据稀疏性问题导致基于词的模型需要学习更多参数，因此更容易过拟合。
	- 作者在BQ数据集上进行了实验，结果证实了这一观点。为了取得最好的效果，基于词的模型（0.5）需要比基于字符的模型（0.3）使用更大的dropout rate。这意味着对基于词的模型而言，过拟合的问题更加地严峻。
	- 作者还发现，与基于字符的模型相比，基于词的模型中具有不同dropout rate的曲线更接近在一起，这意味着dropout 不足以解决它的过拟合问题。 与具有最佳dropout rate的基于词的模型（80.65）相比，没有使用dropout的基于字符的模型已经实现了更好的性能（80.82）。
4. 可视化
	- ![[Pasted image 20201123185246.png]]
	- 因为“利息费用”没被分开，所以无法与利息对应上